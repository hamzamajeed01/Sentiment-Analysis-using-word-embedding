{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f56a5b-09f6-4653-a89d-f5fcaec27ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e37add01-617d-4503-9531-a4a541a36afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 714, 715, 215, 6, 216, 151, 714, 1818, 1819, 1820, 182, 716, 1034, 2, 1821, 715, 1822, 1823], [37, 20, 24, 9, 64, 85, 2, 60, 217], [1824, 1825, 1035, 1036, 417, 1, 1826, 1037, 1827, 1038, 249, 717, 514, 305, 718, 1038], [183, 65, 515, 1039, 106, 106, 77, 418, 516, 1828], [1829, 348, 1830, 306, 517, 47, 1040, 1041, 152, 89, 1831, 152, 1040, 1832, 152, 307, 518, 200]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('urdu-sentiment-corpus-v1.tsv', sep='\\t', header=None, names=['Tweet', 'Class'])\n",
    "data = data[data['Class'].isin(['P', 'N'])] \n",
    "def preprocess_text(text, stop_words):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "urdu_stopwords = ['کا', 'کی', 'ہے', 'میں',\"اور\", \"کے\", \"میں\", \"کی\", \"ہے\", \"یہ\", \"پر\", \"سے\", \"ہیں\", \"کو\",\"ہو\",\"وہ\", \"اس\", \"کا\", \"جو\", \"کر\", \"تھا\", \"تو\", \"ہوں\", \"گی\"]\n",
    "data['Preprocessed_Tweet'] = data['Tweet'].apply(lambda x: preprocess_text(x, urdu_stopwords))\n",
    "label_mapping = {'P': 1, 'N': 0}\n",
    "data['Class'] = data['Class'].map(label_mapping)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Preprocessed_Tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(data['Preprocessed_Tweet'])\n",
    "print(sequences[:5])\n",
    "max_sequence_length = max(len(x) for x in sequences)\n",
    "data_padded = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "375b5241-aee7-4dae-b21c-a6027c5b78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: 734\n",
      "Shape of X_test: 245\n",
      "Shape of y_train: 734\n",
      "Shape of y_test: 245\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_padded, data['Class'].values, test_size=0.25, random_state=42)\n",
    "print(\"Shape of X_train:\", len(X_train))\n",
    "print(\"Shape of X_test:\", len(X_test))\n",
    "print(\"Shape of y_train:\", len(y_train))\n",
    "print(\"Shape of y_test:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc112963-8457-4ed3-a479-50b8f735aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Coding_Softwares\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 - 40s - 6s/step - accuracy: 0.5134 - loss: 0.6962 - val_accuracy: 0.5306 - val_loss: 0.6924\n",
      "Epoch 2/10\n",
      "7/7 - 8s - 1s/step - accuracy: 0.5198 - loss: 0.6873 - val_accuracy: 0.5510 - val_loss: 0.6877\n",
      "Epoch 3/10\n",
      "7/7 - 5s - 769ms/step - accuracy: 0.8736 - loss: 0.5513 - val_accuracy: 0.5051 - val_loss: 1.0302\n",
      "Epoch 4/10\n",
      "7/7 - 5s - 736ms/step - accuracy: 0.8863 - loss: 0.2969 - val_accuracy: 0.5969 - val_loss: 0.8820\n",
      "Epoch 5/10\n",
      "7/7 - 6s - 822ms/step - accuracy: 0.9553 - loss: 0.1392 - val_accuracy: 0.5867 - val_loss: 1.1402\n",
      "Epoch 6/10\n",
      "7/7 - 6s - 811ms/step - accuracy: 0.9808 - loss: 0.0684 - val_accuracy: 0.6224 - val_loss: 1.8417\n",
      "Epoch 7/10\n",
      "7/7 - 5s - 743ms/step - accuracy: 0.9872 - loss: 0.0450 - val_accuracy: 0.6224 - val_loss: 1.9145\n",
      "Epoch 8/10\n",
      "7/7 - 5s - 785ms/step - accuracy: 0.9923 - loss: 0.0191 - val_accuracy: 0.6071 - val_loss: 1.5946\n",
      "Epoch 9/10\n",
      "7/7 - 5s - 756ms/step - accuracy: 0.9962 - loss: 0.0153 - val_accuracy: 0.6224 - val_loss: 1.4291\n",
      "Epoch 10/10\n",
      "7/7 - 10s - 1s/step - accuracy: 0.9974 - loss: 0.0092 - val_accuracy: 0.6071 - val_loss: 1.5023\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 128, input_shape=(max_sequence_length,)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(data_padded, data['Class'], batch_size=128, epochs=10, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2f754c-7df9-42bc-a8af-1d6a842eaea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 744ms/step\n",
      "Accuracy: 0.9183673469387755\n",
      "Precision (Positive): 0.9439252336448598\n",
      "Recall (Positive): 0.8782608695652174\n",
      "F1-Score (Positive): 0.9099099099099099\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'], output_dict=True)\n",
    "print(\"Accuracy:\", report['accuracy'])\n",
    "print(\"Precision (Positive):\", report['Positive']['precision'])\n",
    "print(\"Recall (Positive):\", report['Positive']['recall'])\n",
    "print(\"F1-Score (Positive):\", report['Positive']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64225c50-84b0-44b7-b52a-b6c844fccf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_file_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_embeddings = KeyedVectors.load_word2vec_format(word2vec_file_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea2bc57-86b1-42e2-9431-72a889fe7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))  \n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_embeddings:\n",
    "        embedding_matrix[i] = word2vec_embeddings[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22301d80-46ae-4145-8550-7bd164ccc659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Coding_Softwares\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,497,900</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">439,296</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m1,497,900\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_15 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m439,296\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_16 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m394,240\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_17 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m394,240\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,725,933</span> (10.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,725,933\u001b[0m (10.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,725,933</span> (10.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,725,933\u001b[0m (10.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 300,  # Assuming Word2Vec embeddings are 300-dimensional\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_shape=(max_sequence_length,),\n",
    "                    trainable=False))  # We don't want to train these embeddings\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f9397d-f8ce-4bcd-8ff5-aca99d2bc904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 - 7s - 1s/step - accuracy: 0.9986 - loss: 0.0104 - val_accuracy: 0.6245 - val_loss: 2.0093\n",
      "Epoch 2/10\n",
      "6/6 - 10s - 2s/step - accuracy: 0.9986 - loss: 0.0130 - val_accuracy: 0.6245 - val_loss: 2.0940\n",
      "Epoch 3/10\n",
      "6/6 - 7s - 1s/step - accuracy: 0.9986 - loss: 0.0098 - val_accuracy: 0.6245 - val_loss: 2.1413\n",
      "Epoch 4/10\n",
      "6/6 - 7s - 1s/step - accuracy: 0.9986 - loss: 0.0096 - val_accuracy: 0.6245 - val_loss: 2.1756\n",
      "Epoch 5/10\n",
      "6/6 - 10s - 2s/step - accuracy: 0.9986 - loss: 0.0095 - val_accuracy: 0.6245 - val_loss: 2.2099\n",
      "Epoch 6/10\n",
      "6/6 - 10s - 2s/step - accuracy: 0.9986 - loss: 0.0095 - val_accuracy: 0.6245 - val_loss: 2.2330\n",
      "Epoch 7/10\n",
      "6/6 - 11s - 2s/step - accuracy: 0.9986 - loss: 0.0094 - val_accuracy: 0.6204 - val_loss: 2.2593\n",
      "Epoch 8/10\n",
      "6/6 - 7s - 1s/step - accuracy: 0.9986 - loss: 0.0094 - val_accuracy: 0.6204 - val_loss: 2.2859\n",
      "Epoch 9/10\n",
      "6/6 - 6s - 1s/step - accuracy: 0.9986 - loss: 0.0093 - val_accuracy: 0.6204 - val_loss: 2.3160\n",
      "Epoch 10/10\n",
      "6/6 - 6s - 1s/step - accuracy: 0.9986 - loss: 0.0092 - val_accuracy: 0.6204 - val_loss: 2.3334\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.6350 - loss: 2.2722\n",
      "Test Loss: 2.337270498275757\n",
      "Test Accuracy: 0.6204081773757935\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 520ms/step\n",
      "Classification Report:\n",
      "{'Negative': {'precision': 0.6390977443609023, 'recall': 0.6538461538461539, 'f1-score': 0.6463878326996199, 'support': 130}, 'Positive': {'precision': 0.5982142857142857, 'recall': 0.5826086956521739, 'f1-score': 0.5903083700440528, 'support': 115}, 'accuracy': 0.6204081632653061, 'macro avg': {'precision': 0.6186560150375939, 'recall': 0.6182274247491639, 'f1-score': 0.6183481013718364, 'support': 245}, 'weighted avg': {'precision': 0.6199075494859599, 'recall': 0.6204081632653061, 'f1-score': 0.6200648196163945, 'support': 245}}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_padded, data['Class'].values, test_size=0.25, random_state=42)\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test), verbose=2)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "report = classification_report(y_test, y_pred_classes, target_names=['Negative', 'Positive'], output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ae4e633-7eb3-4e20-8a3f-10ef297b535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6204081632653061\n",
      "Precision (Positive): 0.5982142857142857\n",
      "Recall (Positive): 0.5826086956521739\n",
      "F1-Score (Positive): 0.5903083700440528\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", report['accuracy'])\n",
    "print(\"Precision (Positive):\", report['Positive']['precision'])\n",
    "print(\"Recall (Positive):\", report['Positive']['recall'])\n",
    "print(\"F1-Score (Positive):\", report['Positive']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8854ea6-1932-4873-ad8e-1835321f8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings_path = 'glove.6B.200d.txt'\n",
    "glove_embeddings_index = load_glove_embeddings(glove_embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24eb766d-5a83-4956-bc8f-9c3dedda341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 - 40s - 7s/step - accuracy: 0.5027 - loss: 0.6937 - val_accuracy: 0.5469 - val_loss: 0.6904\n",
      "Epoch 2/10\n",
      "6/6 - 7s - 1s/step - accuracy: 0.5450 - loss: 0.6799 - val_accuracy: 0.5143 - val_loss: 0.6906\n",
      "Epoch 3/10\n",
      "6/6 - 9s - 1s/step - accuracy: 0.7180 - loss: 0.5757 - val_accuracy: 0.5959 - val_loss: 0.6757\n",
      "Epoch 4/10\n",
      "6/6 - 5s - 761ms/step - accuracy: 0.9223 - loss: 0.2480 - val_accuracy: 0.6327 - val_loss: 1.1597\n",
      "Epoch 5/10\n",
      "6/6 - 5s - 787ms/step - accuracy: 0.9823 - loss: 0.0710 - val_accuracy: 0.6204 - val_loss: 1.5259\n",
      "Epoch 6/10\n",
      "6/6 - 4s - 652ms/step - accuracy: 0.9850 - loss: 0.0569 - val_accuracy: 0.6122 - val_loss: 1.3464\n",
      "Epoch 7/10\n",
      "6/6 - 4s - 656ms/step - accuracy: 0.9850 - loss: 0.0429 - val_accuracy: 0.6041 - val_loss: 1.5852\n",
      "Epoch 8/10\n",
      "6/6 - 4s - 638ms/step - accuracy: 0.9959 - loss: 0.0182 - val_accuracy: 0.6122 - val_loss: 1.3678\n",
      "Epoch 9/10\n",
      "6/6 - 4s - 651ms/step - accuracy: 0.9946 - loss: 0.0137 - val_accuracy: 0.6204 - val_loss: 1.3437\n",
      "Epoch 10/10\n",
      "6/6 - 5s - 858ms/step - accuracy: 0.9946 - loss: 0.0136 - val_accuracy: 0.6286 - val_loss: 1.3944\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.6535 - loss: 1.3896\n",
      "Test Loss: 1.399660587310791\n",
      "Test Accuracy: 0.6285714507102966\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001944C90FB00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 242ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.58      0.63       130\n",
      "    Positive       0.59      0.68      0.63       115\n",
      "\n",
      "    accuracy                           0.63       245\n",
      "   macro avg       0.63      0.63      0.63       245\n",
      "weighted avg       0.63      0.63      0.63       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dim = 200  \n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, input_shape=(max_sequence_length,), trainable=False))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a15f4c0-4c49-4935-9ac9-9e1577739630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nitk\n",
      "  Downloading nitk-0.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading nitk-0.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: nitk\n",
      "Successfully installed nitk-0.1\n",
      "Requirement already satisfied: gensim in e:\\coding_softwares\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in e:\\coding_softwares\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in e:\\coding_softwares\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\coding_softwares\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pandas in e:\\coding_softwares\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.1.4)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\coding_softwares\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\coding_softwares\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in e:\\coding_softwares\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions in e:\\coding_softwares\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\coding_softwares\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/59.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.6 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 30.7/59.6 kB 100.9 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 41.0/59.6 kB 35.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 51.2/59.6 kB 35.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.6/59.6 kB 42.1 kB/s eta 0:00:00\n",
      "Downloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20451 sha256=902bd2e5c1e458306c85d1390f60ebcfb46983d868d1f9edcf98a2b5fa6c75aa\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3517 sha256=daf4ffcc298a73f73df77473f46aba720353cef4b1d9bdf37424d8b57e7dc4fb\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.3.1 simpful-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nitk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00167359-7f56-47a1-b9f5-7861be9a3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd64ed14-7e60-4e1b-b7a5-5254551dbff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_1/bidirectional_36/forward_lstm_36/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_1/bidirectional_36/forward_lstm_36/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_1/bidirectional_36/backward_lstm_36/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_1/bidirectional_36/backward_lstm_36/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_3/bidirectional_37/forward_lstm_37/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_3/bidirectional_37/forward_lstm_37/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_3/bidirectional_37/backward_lstm_37/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_3/bidirectional_37/backward_lstm_37/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_5/bidirectional_38/forward_lstm_38/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_5/bidirectional_38/forward_lstm_38/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_5/bidirectional_38/backward_lstm_38/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'sequential_16_5/bidirectional_38/backward_lstm_38/lstm_cell/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_array_ops.py\", line 316, in diag\n",
      "    control_flow_assert.Assert(  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 155, in error_handler\n",
      "    del filtered_tb  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)  File \"E:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32mE:\\Coding_Softwares\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\Coding_Softwares\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:503\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_padded, data['Class'].values, test_size=0.25, random_state=42)\n",
    "\n",
    "tokenized_tweets = [word_tokenize(tweet) for tweet in data['Tweet']]\n",
    "fasttext_model = FastText(sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Create embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "# Define and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 100, input_shape=(max_sequence_length,), trainable=False))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "833f5936-5317-4cdd-82e6-ab72df863a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in e:\\coding_softwares\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\coding_softwares\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\coding_softwares\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in e:\\coding_softwares\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\coding_softwares\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\coding_softwares\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: tensorflow-hub in e:\\coding_softwares\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-hub) (3.20.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-hub) (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in e:\\coding_softwares\\lib\\site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
      "Requirement already satisfied: packaging in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (23.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
      "Requirement already satisfied: setuptools in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in e:\\coding_softwares\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\coding_softwares\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.41.2)\n",
      "Requirement already satisfied: rich in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (13.3.5)\n",
      "Requirement already satisfied: namex in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.0.7)\n",
      "Requirement already satisfied: optree in e:\\coding_softwares\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\coding_softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\coding_softwares\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\coding_softwares\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in e:\\coding_softwares\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\coding_softwares\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\coding_softwares\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323497d-d112-4608-9a30-f2b8a2c38834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_padded, data['Class'].values, test_size=0.25, random_state=42)\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "X_train_elmo = elmo(X_train, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "X_test_elmo = elmo(X_test, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "# Define and compile the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(None, 1024)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_elmo, y_train, batch_size=128, epochs=10, validation_data=(X_test_elmo, y_test), verbose=2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_elmo, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = (model.predict(X_test_elmo) > 0.5).astype(\"int32\")\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc546634-44fd-493f-9a71-e4d5504817f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb5923-99b2-4fa9-b179-610d23b32eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f4d31-8769-4846-88f2-eeb05275f92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
